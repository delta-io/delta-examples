{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4067f98-8b49-4931-acdb-05f6565bf2fa",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "This notebook uses [Evcxr Jupyter Kernel](https://github.com/evcxr/evcxr/tree/main/evcxr_jupyter) to run Rust code in a Jupyter notebook. Follow the instructions in [Evcxr Jupyter Kernel](https://github.com/evcxr/evcxr/tree/main/evcxr_jupyter) to build the environment, or just use a Docker file included here (this could be your only option if you are on MacOS with Apple chip becuase of linking errors)\n",
    "```sh\n",
    "docker build -t delta-rs .\n",
    "docker run -it --rm  -p 8888:8888 --name delta-rs -v $PWD/notebooks/delta-rs:/usr/src/delta-rs delta-rs\n",
    "```\n",
    "\n",
    "The `Evcxr Jupyter Kernel` seem to have some limitations when it comes to running async code (some work, some don't), so this notebook uses `tokio::runtime::Runtime.block_on` to handle most of the async calls, which should not be necessary if you are running the same code directly (not through Jupyter).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9b249-d61f-4cda-bf98-6a848fab71a5",
   "metadata": {},
   "source": [
    "# Introduction to delta-rs\n",
    "\n",
    "This notebook introduces you to the key features of Delta Lake via the delta-rs library.\n",
    "\n",
    "`delta-rs` allows you to work with Delta Lake without a Spark runtime.\n",
    "\n",
    "Once you work through this notebook, you'll have a better understanding of the features that make Delta Lake powerful. It's a relatively quick guide and should be eye-opening! Let's dive in!\n",
    "\n",
    "We'll start by installing dependencies and importing required types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "409e1b65-a306-45b6-a3bb-554d39d32969",
   "metadata": {},
   "outputs": [],
   "source": [
    ":dep deltalake = { version = \"0.16.2\", features = [\"arrow\", \"datafusion\"]}\n",
    ":dep tokio = { version = \"1\", features = [\"full\"] }\n",
    ":dep serde = { version = \"1\", features = [\"derive\"] }\n",
    ":dep serde_json = \"1\"\n",
    ":dep chrono = {version = \"0.4.31\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa4ed930-2c0b-4586-9dce-962726b0c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "use std::sync::Arc;\n",
    "\n",
    "use deltalake::arrow::record_batch::RecordBatch;\n",
    "use deltalake::arrow::datatypes::{Field, DataType, Schema as ArrowSchema};\n",
    "use deltalake::arrow::array::{Int32Array, StringArray};\n",
    "use deltalake::arrow::util::pretty::print_batches;\n",
    "\n",
    "use deltalake::DeltaOps;\n",
    "use deltalake::DeltaTable;\n",
    "use deltalake::operations::collect_sendable_stream;\n",
    "use deltalake::writer::{DeltaWriter, RecordBatchWriter, JsonWriter};\n",
    "use deltalake::schema::{Schema, SchemaField, SchemaDataType}\n",
    "use deltalake::datafusion::logical_expr::{col, lit};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c330a6-0d42-4520-90a6-c89f0853cfb5",
   "metadata": {},
   "source": [
    "## Create a Delta Lake\n",
    "\n",
    "Let's create a new Delta Lake table with some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ac43fe-5090-4968-b8d5-dfdac92115e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "let TABLE_URI = \"delta-table\".to_string();\n",
    "let rt = tokio::runtime::Runtime::new().unwrap();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "105b888c-4c87-4c3a-978d-67597444377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "let schema = Arc::new(ArrowSchema::new(\n",
    "    vec![\n",
    "        Field::new(\"num\", DataType::Int32, false),\n",
    "        Field::new(\"letter\", DataType::Utf8, true),\n",
    "    ]\n",
    "));\n",
    "\n",
    "let batch = RecordBatch::try_new(\n",
    "    schema, \n",
    "    vec![\n",
    "        Arc::new(Int32Array::from(vec![1, 2, 3])),\n",
    "        Arc::new(StringArray::from(vec![\"a\", \"b\", \"c\"])),\n",
    "    ]\n",
    ")?;\n",
    "rt.block_on(async {\n",
    "    let ops = DeltaOps::try_from_uri(&TABLE_URI).await.unwrap();\n",
    "    ops.write(vec![batch]).await;\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f5936-51cc-41d8-931e-71a10c3d2db9",
   "metadata": {},
   "source": [
    "You can inspect the contents of the `delta-table` folder to begin understanding how Delta Lake works. Here's what the folder will contain:\n",
    "```\n",
    "delta-table/\n",
    "    _delta_log/\n",
    "      00000000000000000000.json\n",
    "part-00001-b0d48b69-28f8-4301-9876-e6b7e3af9db7-c000.snappy.parquet\n",
    "```\n",
    "\n",
    "`delta-table` contains a `delta_log` folder which is often refered to as the \"transaction log\". The transaction log tracks the files that have been added and removed from the Delta Lake, along with other metadata.\n",
    "\n",
    "The Parquet file contains the actual data that was written to the Delta Lake.\n",
    "\n",
    "You don't need to have a detailed understanding of how the transaction log works. A high level conceptual grasp is all you need to understand how Delta Lake provides you with useful data management features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11286513-c67e-45ad-a7ac-2aa96b4d6ba2",
   "metadata": {},
   "source": [
    "## Read a Delta Lake\n",
    "\n",
    "Let's read and print out the table contents. We are using `datafusion` for this and need to import some supporting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14a37cc6-9405-4278-9fa8-28fb0bb080ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Table version: 0\n",
      "+-----+--------+\n",
      "| num | letter |\n",
      "+-----+--------+\n",
      "| 1   | a      |\n",
      "| 2   | b      |\n",
      "| 3   | c      |\n",
      "+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "fn show_table_data(rt: &tokio::runtime::Runtime, table_name: &str) {\n",
    "    let (table, data) = rt.block_on(async {\n",
    "        let ops = DeltaOps::try_from_uri(table_name).await.unwrap();\n",
    "        let (table, stream) = ops.load().await.unwrap();\n",
    "        let data: Vec<RecordBatch> = collect_sendable_stream(stream).await.unwrap();\n",
    "        (table, data)\n",
    "    });\n",
    "    println!(\"----------------\\nTable version: {}\", table.version());\n",
    "    deltalake::arrow::util::pretty::print_batches(&data);\n",
    "}\n",
    "show_table_data(&rt, &TABLE_URI);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7eb5fb-6f26-4cb5-a1dc-43466ad6a5a3",
   "metadata": {},
   "source": [
    "After the first data insert, the Delta Lake is at \"version 0\". Let's add some more data to the Delta Lake and see how the version gets updated after another write transaction is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f44b6-ef51-4bb8-b3b2-e148c99bd6fe",
   "metadata": {},
   "source": [
    "## Insert more data into Delta Lake\n",
    "\n",
    "Add more data to this table. Let's use a `JsonWriter` this timeIgnore the schema information declared earlier and derive it from the table (though we still use that knowledge to create the actual data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a66260b-5c98-4a62-8753-d220c879361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Table version: 1\n",
      "+-----+--------+\n",
      "| num | letter |\n",
      "+-----+--------+\n",
      "| 1   | a      |\n",
      "| 2   | b      |\n",
      "| 3   | c      |\n",
      "| 77  | x      |\n",
      "| 88  | y      |\n",
      "| 99  | z      |\n",
      "+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "fn write_json(rt: &tokio::runtime::Runtime, table_name: &str, json_data: &str) {\n",
    "    rt.block_on(async {\n",
    "        // Open the table\n",
    "        let mut table = deltalake::open_table(table_name).await.unwrap();\n",
    "    \n",
    "        // Create a writer\n",
    "        let mut writer = JsonWriter::for_table(&table).unwrap();\n",
    "\n",
    "        // Write and commit the changes\n",
    "        writer.write(\n",
    "            json_data\n",
    "                .lines()\n",
    "                .map(|line| serde_json::from_str(line).unwrap())\n",
    "                .collect(),\n",
    "        ).await.unwrap();\n",
    "        writer.flush_and_commit(&mut table).await.unwrap();\n",
    "    });\n",
    "}\n",
    "\n",
    "let json_data = \"{\\\"num\\\" : 77, \\\"letter\\\": \\\"x\\\"}\\n\\\n",
    "                 {\\\"num\\\" : 88, \\\"letter\\\": \\\"y\\\"}\\n\\\n",
    "                 {\\\"num\\\" : 99, \\\"letter\\\": \\\"z\\\"}\";\n",
    "write_json(&rt, &TABLE_URI, json_data);\n",
    "\n",
    "show_table_data(&rt, &TABLE_URI);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df0fe9-42da-4668-a425-cb8bf0150064",
   "metadata": {},
   "source": [
    "Notice how the table version has changed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f7d65-dfa4-48ca-bab0-f018c57077ed",
   "metadata": {},
   "source": [
    "## Time travel to previous version of data\n",
    "Let's travel back in time and inspect the content of the Delta Lake at \"version 0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5609c5cd-c174-4db2-8917-ebd4b49b7bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Table version: 0\n",
      "+-----+--------+\n",
      "| num | letter |\n",
      "+-----+--------+\n",
      "| 1   | a      |\n",
      "| 2   | b      |\n",
      "| 3   | c      |\n",
      "+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "fn show_table_version_data(rt: &tokio::runtime::Runtime, table_name: &str, version: i64) -> DeltaTable {\n",
    "    rt.block_on(async {\n",
    "        let mut table = deltalake::open_table(table_name).await.unwrap();\n",
    "        table.load_version(version).await.unwrap();\n",
    "        let (_table, stream) = DeltaOps(table).load().await.unwrap();\n",
    "        let data: Vec<RecordBatch> = collect_sendable_stream(stream).await.unwrap();\n",
    "        println!(\"----------------\\nTable version: {}\", &_table.version());\n",
    "        deltalake::arrow::util::pretty::print_batches(&data);\n",
    "        _table\n",
    "    })\n",
    "}\n",
    "show_table_version_data(&rt, &TABLE_URI, 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214be5ac-2e86-418a-8896-a19a59af2ae3",
   "metadata": {},
   "source": [
    "Wow! That's cool!\n",
    "\n",
    "We performed two write transactions and were able to travel back in time and view the contents of the Delta Lake before the second write transaction was performed. This is an incredibly powerful and useful feature.\n",
    "\n",
    "Delta Lake gives you time travel for free!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c61eaf-4a61-4bf9-bf64-9394882f300c",
   "metadata": {},
   "source": [
    "## Schema enforcement\n",
    "\n",
    "Schema enforcement is enabled by default. Here we are trying to append a record that does not contain a required `num` field resulting in error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb68fb6-9da9-4c10-bd98-de8e1fdd6c2e",
   "metadata": {},
   "source": [
    "**Note:** if you do the same without specifying the schema then write may work with nulls used for missing columns values if null values are allowed in the current table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8137a207-e877-48f9-91a5-4abe21e4140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at 'called `Result::unwrap()` on an `Err` value: Generic(\"Failed to convert into Arrow schema: Json error: Encountered unmasked nulls in non-nullable StructArray child: Field { name: \\\"num\\\", data_type: Int32, nullable: false, dict_id: 0, dict_is_ordered: false, metadata: {} }\")', src/lib.rs:30:17\n",
      "stack backtrace:\n",
      "   0: rust_begin_unwind\n",
      "             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/std/src/panicking.rs:593:5\n",
      "   1: core::panicking::panic_fmt\n",
      "             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/core/src/panicking.rs:67:14\n",
      "   2: core::result::unwrap_failed\n",
      "             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/core/src/result.rs:1651:5\n",
      "   3: tokio::runtime::park::CachedParkThread::block_on\n",
      "   4: tokio::runtime::context::runtime::enter_runtime\n",
      "   5: tokio::runtime::runtime::Runtime::block_on\n",
      "   6: run_user_code_12\n",
      "   7: evcxr::runtime::Runtime::run_loop\n",
      "   8: evcxr::runtime::runtime_hook\n",
      "   9: evcxr_jupyter::main\n",
      "note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\n"
     ]
    }
   ],
   "source": [
    "let bad_data = \"{\\\"name\\\": [\\\"bob\\\", \\\"denise\\\"], \\\"age\\\": [64, 43]}\";\n",
    "write_json(&rt, &TABLE_URI, bad_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24eaf9-b932-4952-b41e-f798689141c0",
   "metadata": {},
   "source": [
    "**Note:** because we are not explicitly specifying a schema, a write operation using `JsonWriter` may still succeed if the data is compatible with existing schema (e.g. the following will still work but additional fields will be ignored and `letter` column values of the new records will be `null`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2aad821-a3ae-4836-929f-c47dc5b230fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Table version: 2\n",
      "+-----+--------+\n",
      "| num | letter |\n",
      "+-----+--------+\n",
      "| 1   | a      |\n",
      "| 2   | b      |\n",
      "| 3   | c      |\n",
      "| 77  | x      |\n",
      "| 88  | y      |\n",
      "| 99  | z      |\n",
      "| 4   |        |\n",
      "+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "let bad_data = \"{\\\"num\\\": 4, \\\"name\\\": \\\"bob\\\", \\\"age\\\": 64}\";\n",
    "write_json(&rt, &TABLE_URI, bad_data);\n",
    "show_table_data(&rt, &TABLE_URI);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb59e9-a46d-4187-ad11-5b3224cf9c8a",
   "metadata": {},
   "source": [
    "## Delete rows\n",
    "\n",
    "This section demonstrates how you can delete rows of data from the Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9513f533-8c0a-4fcf-8718-d07c630e6a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Table version: 3\n",
      "+-----+--------+\n",
      "| num | letter |\n",
      "+-----+--------+\n",
      "| 2   | b      |\n",
      "| 3   | c      |\n",
      "| 77  | x      |\n",
      "| 88  | y      |\n",
      "+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "rt.block_on(async {\n",
    "    let mut table = deltalake::open_table(&TABLE_URI).await.unwrap();\n",
    "    let (table, _) = DeltaOps(table)\n",
    "        .delete()\n",
    "        .with_predicate(col(\"num\").lt(lit(2)).or(col(\"num\").gt(lit(88))).or(col(\"letter\").is_null()))\n",
    "        .await\n",
    "        .unwrap();\n",
    "}); \n",
    "show_table_data(&rt, &TABLE_URI);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a542d866-1f3d-44d8-9a92-8c1be2d75f60",
   "metadata": {},
   "source": [
    "## Vacuum old data files\n",
    "\n",
    "Delta Lake doesn't delete stale file from disk by default. We just performed an overwrite transaction which means that all the data for the latest version of the Delta Lake is in a new file. When we read in the latest version of the Delta Lake, it'll just read the new file. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06a1e3d9-5d4d-49a1-9d7c-7274e4f3d1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Files:\n",
      "part-00001-a573ca35-c283-4a17-997b-dcbc84e0bda9-c000.snappy.parquet\n",
      "----------------\n",
      "Table version: 3\n",
      "+-----+--------+\n",
      "| num | letter |\n",
      "+-----+--------+\n",
      "| 2   | b      |\n",
      "| 3   | c      |\n",
      "| 77  | x      |\n",
      "| 88  | y      |\n",
      "+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "fn show_table_files(table: &DeltaTable) {\n",
    "    let files: Vec<_> = table.get_files();\n",
    "    let mapped: Vec<&str> = files.iter().map(|p| p.filename().unwrap()).collect();\n",
    "    println!(\"----------------\\nFiles:\\n{}\", mapped.join(\"\\n\"));\n",
    "}\n",
    "fn show_files(rt: &tokio::runtime::Runtime, table_name: &str) {\n",
    "    rt.block_on(async {\n",
    "        let table = deltalake::open_table(table_name).await.unwrap();\n",
    "        show_table_files(&table);\n",
    "    });\n",
    "}\n",
    "show_files(&rt, &TABLE_URI);\n",
    "show_table_data(&rt, &TABLE_URI);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1e80e-f21d-42ed-82f5-7c88b6c9c1a4",
   "metadata": {},
   "source": [
    "We have several Parquet files on disk, but only a subset of them are used by the current version of the Delta Lake. Let's take a look at all the Parquet files currently in the Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5370e83d-0cd8-4649-9519-1efe14d8d7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files\n",
      "\"part-00001-a573ca35-c283-4a17-997b-dcbc84e0bda9-c000.snappy.parquet\"\n",
      "\"part-00000-d6cbb34c-f3d1-432c-952d-5fa1eb83b91a-c000.snappy.parquet\"\n",
      "\"part-00001-94474cb5-27f8-43db-8570-7ed4030f6e64-c000.snappy.parquet\"\n",
      "\"part-00000-a4dcafbf-a520-43d2-a0c8-6ea38bec26ff-c000.snappy.parquet\"\n"
     ]
    }
   ],
   "source": [
    "fn show_files_in_table_dir(table_uri: &str) {\n",
    "    let to_list = format!(\"./{}\", table_uri);\n",
    "    let paths = std::fs::read_dir(to_list).unwrap();\n",
    "    println!(\"All files\");\n",
    "    for path in paths {\n",
    "        let p = path.unwrap().path();\n",
    "        if p.is_file() {\n",
    "            println!(\"{:?}\", p.file_name().unwrap());\n",
    "        }\n",
    "    }\n",
    "}\n",
    "show_files_in_table_dir(&TABLE_URI);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ac879-2428-45f8-ba88-3479c12c3bd7",
   "metadata": {},
   "source": [
    "The \"stale\" Parquet files are what allow for time travel. Let's time travel back to \"version 1\" of the Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "959f9de6-a774-4680-a301-56017f76b482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Table version: 1\n",
      "+-----+--------+\n",
      "| num | letter |\n",
      "+-----+--------+\n",
      "| 1   | a      |\n",
      "| 2   | b      |\n",
      "| 3   | c      |\n",
      "| 77  | x      |\n",
      "| 88  | y      |\n",
      "| 99  | z      |\n",
      "+-----+--------+\n",
      "----------------\n",
      "Files:\n",
      "part-00001-94474cb5-27f8-43db-8570-7ed4030f6e64-c000.snappy.parquet\n",
      "part-00000-a4dcafbf-a520-43d2-a0c8-6ea38bec26ff-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "show_table_files(&show_table_version_data(&rt, &TABLE_URI, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e42e4-481a-4aa6-8b2e-66619bd52269",
   "metadata": {},
   "source": [
    "When we time travel back to version 1, we're reading entirely different files than when we read the latest version of the the Delta Lake.\n",
    "\n",
    "The legacy files are what allow you to time travel.\n",
    "\n",
    "If you don't want to time travel, you can delete the legacy files with the vacuum() command.\n",
    "\n",
    "Let's start with running vacuum in \"dry run\" mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbe31f25-59ad-4d56-aad5-09c22c6aae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VacuumMetrics { dry_run: true, files_deleted: [\"part-00000-d6cbb34c-f3d1-432c-952d-5fa1eb83b91a-c000.snappy.parquet\", \"part-00001-94474cb5-27f8-43db-8570-7ed4030f6e64-c000.snappy.parquet\", \"part-00000-a4dcafbf-a520-43d2-a0c8-6ea38bec26ff-c000.snappy.parquet\"] }\n"
     ]
    }
   ],
   "source": [
    "fn run_vacuum(rt: &tokio::runtime::Runtime, table_name: &str, dry_run: bool) {\n",
    "    rt.block_on(async {\n",
    "        let mut table = deltalake::open_table(table_name).await.unwrap();\n",
    "        let (_table, metrics) = DeltaOps(table)\n",
    "            .vacuum()\n",
    "            .with_retention_period(chrono::Duration::zero())\n",
    "            .with_enforce_retention_duration(false)\n",
    "            .with_dry_run(dry_run)\n",
    "            .await\n",
    "            .unwrap();\n",
    "        println!(\"{:?}\", metrics);\n",
    "    })\n",
    "}   \n",
    "run_vacuum(&rt, &TABLE_URI, true);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63d8b7-b69b-464c-bd03-cb2a51cbfc73",
   "metadata": {},
   "source": [
    "The files aren't actually deleted when the code is executed in dry run mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62626b55-b7d0-43f6-ad05-5dd860c1a7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files\n",
      "\"part-00001-a573ca35-c283-4a17-997b-dcbc84e0bda9-c000.snappy.parquet\"\n",
      "\"part-00000-d6cbb34c-f3d1-432c-952d-5fa1eb83b91a-c000.snappy.parquet\"\n",
      "\"part-00001-94474cb5-27f8-43db-8570-7ed4030f6e64-c000.snappy.parquet\"\n",
      "\"part-00000-a4dcafbf-a520-43d2-a0c8-6ea38bec26ff-c000.snappy.parquet\"\n"
     ]
    }
   ],
   "source": [
    "show_files_in_table_dir(&TABLE_URI);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405eb97d-6453-458e-82f8-b3c7907ed463",
   "metadata": {},
   "source": [
    "Explicitly set dry_run to False to actually delete the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bbcafb5-858b-4163-bfc0-88f0ae95a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VacuumMetrics { dry_run: false, files_deleted: [\"part-00000-d6cbb34c-f3d1-432c-952d-5fa1eb83b91a-c000.snappy.parquet\", \"part-00001-94474cb5-27f8-43db-8570-7ed4030f6e64-c000.snappy.parquet\", \"part-00000-a4dcafbf-a520-43d2-a0c8-6ea38bec26ff-c000.snappy.parquet\"] }\n",
      "All files\n",
      "\"part-00001-a573ca35-c283-4a17-997b-dcbc84e0bda9-c000.snappy.parquet\"\n",
      "----------------\n",
      "Table version: 3\n",
      "+-----+--------+\n",
      "| num | letter |\n",
      "+-----+--------+\n",
      "| 2   | b      |\n",
      "| 3   | c      |\n",
      "| 77  | x      |\n",
      "| 88  | y      |\n",
      "+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "run_vacuum(&rt, &TABLE_URI, false);\n",
    "show_files_in_table_dir(&TABLE_URI);\n",
    "show_table_data(&rt, &TABLE_URI);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b870fda4-487e-47e6-b3cb-fe466d0a4de6",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Let's delete the Delta Lake now that we're done with this demo. **Be careful**, make sure it is deleting the table folder only, uncomment the last line of code once you are sure it is deleting a right thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3afdd189-3b5f-41d0-8a40-8b26ee869b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will delete the following folder: ./delta-table\n"
     ]
    }
   ],
   "source": [
    "let to_delete = format!(\"./{}\", &TABLE_URI);\n",
    "println!(\"This will delete the following folder: {}\", to_delete);\n",
    "std::fs::remove_dir_all(to_delete)?;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cdf8a7-2ca8-43ee-b0de-d401029bda58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

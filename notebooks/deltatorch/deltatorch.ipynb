{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deltatorch Example\n",
    "This notebook briefly demonstrates how to use the [deltatorch](https://github.com/delta-incubator/deltatorch) python library to use `DeltaLake` tables as a data source for model training using PyTorch. The deltatorch library allows users to create a PyTorch `DataLoader` from a DeltaLake dataset.\n",
    "\n",
    "In this end-to-end example, we'll create a `DataLoader` from a Delta Lake table containing the MNIST dataset, and use it to train a PyTorch CNN model. The `deltatorch` library enables creating PyTorch `DataLoader`s directly from Delta Lake tables, avoiding any conversion to Pandas/NumPy. This allows efficient, scalable data loading from Delta Lake into PyTorch.\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/delta-incubator/deltatorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"deltatorch-example\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some Data\n",
    "To provide an example with real data, we'll download the `mnist` dataset and save it as a Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from pyspark.sql.types import IntegerType, StructType, StructField, FloatType, BinaryType\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.MNIST(root='./data', train=True, download=True)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "# Convert the data to a Spark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"label\", FloatType(), False),\n",
    "    StructField(\"features\", BinaryType(), False)  # Changed ArrayType(IntegerType()) to BinaryType()\n",
    "])\n",
    "\n",
    "# Convert images to numpy arrays and save as binary\n",
    "train_data = [(i, float(y), bytearray(np.array(x))) for i, (x, y) in enumerate(train_set)]\n",
    "train_df = spark.createDataFrame(train_data, schema).repartition(50)\n",
    "\n",
    "test_data = [(i, float(y), bytearray(np.array(x))) for i, (x, y) in enumerate(test_set)]\n",
    "test_df = spark.createDataFrame(test_data, schema).repartition(50)\n",
    "\n",
    "# Write the DataFrame to Delta Lake format\n",
    "train_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"./data/mnist_delta/train\")\n",
    "test_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"./data/mnist_delta/test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reassure ourselves that we can retrieve the images and labels from the Delta Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview one of the images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select one row from the DataFrame\n",
    "row = train_df.filter(train_df.id == 7).first()\n",
    "\n",
    "# Extract the image data and label\n",
    "image_data = row['features']\n",
    "label = row['label']\n",
    "\n",
    "# Convert the binary data back to a NumPy array and reshape it\n",
    "image_array = np.frombuffer(image_data, dtype=np.uint8).reshape(28, 28)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image_array, cmap='gray')\n",
    "plt.title(f'Label: {label}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a `DataLoader` from Delta Lake Tables\n",
    "We'll create a PyTorch `DataLoader` that reads data directly from the Delta Lake format using `deltatorch`. `deltatorch` creates an iterable dataset out of the delta tables, then wraps it with a PyTorch DataLoader to enable efficient, parallel data loading into a PyTorch model. This avoids the need to convert the full Delta Lake table to Pandas/NumPy before creating the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltatorch import create_pytorch_dataloader\n",
    "from deltatorch import FieldSpec\n",
    "from utils import BinaryToFloatTensor\n",
    "\n",
    "def create_data_loader(path:str, batch_size:int):\n",
    "    return create_pytorch_dataloader(\n",
    "        path,\n",
    "        id_field=\"id\",\n",
    "        fields=[\n",
    "            FieldSpec(\"features\", transform=BinaryToFloatTensor()),\n",
    "            FieldSpec(\"label\"),\n",
    "        ],\n",
    "        num_workers=4,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "train_dl = create_data_loader(\"./data/mnist_delta/train\", batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put the `deltatorch` `DataLoader` to work! We'll train a simple CNN on the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the network architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(32 * 14 * 14, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, 32 * 14 * 14)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the network, loss function and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(2):  # Loop over the dataset multiple times\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress = tqdm(enumerate(train_dl), total=len(train_dl))\n",
    "    for i, data in progress:\n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data['features'], data['label']\n",
    "        inputs = inputs.unsqueeze(1).to(device)  # Add an extra dimension for the single channel (grayscale)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.long())  # Use long() to ensure the labels are of the correct type\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.long()).sum().item()\n",
    "\n",
    "        if (i + 1) % 100 == 0:  # Print training accuracy every 100 batches\n",
    "            acc = 100 * correct / total\n",
    "            progress.set_description(f\"Loss: {loss.item():.4f}, Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss (Train): {loss.item():.4f}, Accuracy (Train): {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll check the accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = create_data_loader(\"./data/mnist_delta/test\", batch_size=32)\n",
    "\n",
    "test_dl = create_data_loader(\"./data/mnist_delta/test\", batch_size=32)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(y_pred, y_test):\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    total = y_test.size(0)\n",
    "    correct = (predicted == y_test).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "total_accuracy = 0\n",
    "\n",
    "with torch.no_grad():  # We don't need gradients for evaluation\n",
    "    progress = tqdm(enumerate(test_dl), total=len(test_dl))\n",
    "    for i, data in progress:\n",
    "        inputs, labels = data['features'], data['label']\n",
    "        inputs = inputs.unsqueeze(1).to(device)  # Add an extra dimension for the single channel (grayscale)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = calculate_accuracy(outputs, labels.long())  # Use long() to ensure the labels are of the correct type\n",
    "        total_accuracy += acc\n",
    "        \n",
    "        #progress.set_description(f\"Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# Calculate average accuracy over the entire test set\n",
    "average_accuracy = total_accuracy / len(test_dl)\n",
    "\n",
    "print(f\"Average test accuracy: {average_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples, see the [examples directory in the deltatorch repository](https://github.com/delta-incubator/deltatorch/tree/main/examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

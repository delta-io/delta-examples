{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9945a68-c777-4a7f-808c-7aebfd4a619a",
   "metadata": {},
   "source": [
    "# Delta Lake Deletion Vectors\n",
    "\n",
    "This notebook demonstrates the performance of deletion vectors by comparing the performance of the same operations on a Delta table without deletion vectors enabled and another Delta table with deletion vectors enabled.  Here are the operations performed:\n",
    "\n",
    "* a delete operation on a row that's in few files\n",
    "* a delete operation on a row that's in many files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c7dc705-d5cf-4049-9cbf-eba39c44a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import delta\n",
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2bd38b0-b326-4611-bd27-e06a87b58757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/matthew.powers/opt/miniconda3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/matthew.powers/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/matthew.powers/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e31ade8f-7ae7-4f65-b053-4481de1c6685;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 90ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e31ade8f-7ae7-4f65-b053-4481de1c6685\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "24/10/05 16:45:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.executor.memory\", \"10G\")\n",
    "    .config(\"spark.driver.memory\", \"25G\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d20e5f9e-8e38-4d02-bf90-5e981d7dab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load(f\"{Path.home()}/data/G1_1e9_1e2_0_0.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf7ddd1-9607-47c2-ad62-6e7e0cc26c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+---+---+-------+---+---+---------+\n",
      "|  id1|  id2|         id3|id4|id5|    id6| v1| v2|       v3|\n",
      "+-----+-----+------------+---+---+-------+---+---+---------+\n",
      "|id016|id059|id0009584273| 31| 54|7579268|  5|  2|92.709317|\n",
      "|id039|id028|id0008226858| 32| 73|1462759|  3| 14|23.308717|\n",
      "|id047|id073|id0004357983| 52| 71| 354157|  2| 15|98.462728|\n",
      "|id043|id069|id0006903604| 37| 35| 372382|  5|  5|32.566149|\n",
      "|id054|id095|id0005719264| 94| 99|6957127|  5| 11| 97.89284|\n",
      "|id029|id027|id0007119528| 11| 41|6768037|  2|  7|26.394021|\n",
      "|id047|id053|id0003186028| 93| 64|3300443|  3| 14|79.319642|\n",
      "|id091|id097|id0007718026| 22| 50|3609381|  5| 15|94.510853|\n",
      "|id090|id033|id0007857423|  5| 65|3618630|  1|  5| 1.579951|\n",
      "|id070|id062|id0001399833| 90| 99|6131090|  5| 14|24.892749|\n",
      "|id039|id030|id0000654974| 22| 18|1298417|  1| 10|15.321252|\n",
      "|id023|id095|id0005131426| 52| 38|7811474|  4| 12| 25.65414|\n",
      "|id070|id013|id0009420524| 14| 33|7075062|  5| 14|43.468912|\n",
      "|id022|id026|id0005191924| 25| 87|4159338|  4|  8| 6.488546|\n",
      "|id020|id032|id0002313922| 84| 11|9165684|  3|  9|16.512325|\n",
      "|id078|id022|id0009474349| 86| 61|6816212|  4| 15|79.239123|\n",
      "|id024|id079|id0008311106| 57| 69|6830494|  3|  9|83.805413|\n",
      "|id053|id073|id0000741477| 95|  4|6277942|  4| 14| 81.13272|\n",
      "|id058|id002|id0003539478| 67|  2|1723400|  4|  8|  5.08792|\n",
      "|id095|id089|id0004424930| 87| 20| 137880|  2| 15|32.932692|\n",
      "+-----+-----+------------+---+---+-------+---+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89e48c-b60d-4414-9719-a99b2a8c0112",
   "metadata": {},
   "source": [
    "## Baseline - Delta table without deletion vectors enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7271cd2-d5bf-448f-87e0-b2cc172dc7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = f\"{Path.home()}/data/delta_baseline_G1_1e9_1e2_0_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76e30c90-ebf3-400a-8441-c3abea24909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 330 ms, sys: 116 ms, total: 447 ms\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df.write.format(\"delta\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56569d6d-cfdc-4a47-905f-4927f7651b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 646 ms, sys: 214 ms, total: 860 ms\n",
      "Wall time: 10min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "delta_table.optimize().executeZOrderBy(\"id1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efd06424-5c29-4e64-a97f-9298c0c76f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_files_<1mb': 0,\n",
       " 'num_files_1mb-500mb': 0,\n",
       " 'num_files_500mb-1gb': 1,\n",
       " 'num_files_1gb-2gb': 24,\n",
       " 'num_files_>2gb': 0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = deltalake.DeltaTable(delta_path)\n",
    "levi.delta_file_sizes(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af0be0a3-61e5-4955-9929-fe70b6ca693d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_files': 25, 'num_files_skipped': 24, 'num_bytes_skipped': 25997502904}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levi.skipped_stats(dt, filters=[(\"id1\", \"=\", \"'id001'\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e54a1-79b8-4976-9be2-6509992deef9",
   "metadata": {},
   "source": [
    "### Computations before deletion vectors are enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cd559c6-02c1-4338-842e-f47be892a21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.4 ms, sys: 3.96 ms, total: 10.4 ms\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "delta_table = delta.DeltaTable.forPath(spark, delta_path)\n",
    "delta_table.delete(F.col(\"id1\") == \"id001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3dc60fcc-2f7b-4e04-9298-7ddde465828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(delta_path).createOrReplaceTempView(\"x0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8e1301e-b391-4e7e-bb18-89b4adb5e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 126:====================================================>(203 + 3) / 206]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|  id1|         v1|\n",
      "+-----+-----------+\n",
      "|id089|2.9990077E7|\n",
      "|id087|2.9997379E7|\n",
      "|id086|3.0003608E7|\n",
      "|id085|3.0010513E7|\n",
      "|id088|2.9999642E7|\n",
      "|id013|2.9989026E7|\n",
      "|id014|2.9998476E7|\n",
      "|id016|3.0003304E7|\n",
      "|id017|2.9995061E7|\n",
      "|id015|3.0006177E7|\n",
      "|id023|2.9988818E7|\n",
      "|id021|2.9982118E7|\n",
      "|id025|3.0016745E7|\n",
      "|id022|2.9994847E7|\n",
      "|id024|3.0003956E7|\n",
      "|id051|2.9994785E7|\n",
      "|id052|3.0014118E7|\n",
      "|id053| 2.999236E7|\n",
      "|id050|3.0008271E7|\n",
      "|id049|2.9978475E7|\n",
      "+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 68.9 ms, sys: 23.1 ms, total: 92 ms\n",
      "Wall time: 19.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\"select id1, sum(v1) as v1 from x0 group by id1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8952620-56eb-4f00-b7f9-0d9aa97f7638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 219 ms, sys: 70.5 ms, total: 289 ms\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "delta_table = delta.DeltaTable.forPath(spark, delta_path)\n",
    "delta_table.delete((F.col(\"id2\") == F.lit(\"id030\")) & (F.col(\"id4\") == F.lit(22)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01a4203f-bebe-44e0-9b9c-5a78c22cd73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 146:====================================================>(203 + 2) / 205]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|  id2|     sum_id4|\n",
      "+-----+------------+\n",
      "|id089|4.99702831E8|\n",
      "|id080|5.00174365E8|\n",
      "|id087|4.99700307E8|\n",
      "|id073|4.99578553E8|\n",
      "|id043|4.99827245E8|\n",
      "|id064|4.99800549E8|\n",
      "|id051|4.99712039E8|\n",
      "|id045|4.99602129E8|\n",
      "|id074|4.99709236E8|\n",
      "|id023|5.00068708E8|\n",
      "|id006|  4.999594E8|\n",
      "|id013| 5.0012219E8|\n",
      "|id055|4.99983587E8|\n",
      "|id099| 4.9972291E8|\n",
      "|id056| 5.0021648E8|\n",
      "|id052|4.99804639E8|\n",
      "|id093|4.99838447E8|\n",
      "|id034|4.99737564E8|\n",
      "|id075|4.99923346E8|\n",
      "|id036|4.99873408E8|\n",
      "+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 58.3 ms, sys: 18.3 ms, total: 76.6 ms\n",
      "Wall time: 21.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\"select id2, sum(id4) as sum_id4 from x0 group by id2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352006a-1576-4736-bd48-f9ae7480c6b7",
   "metadata": {},
   "source": [
    "## Computations with deletion vectors enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a0f4da1-49b8-4a36-ae9a-80dca2244e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = f\"{Path.home()}/data/delta_dv_G1_1e9_1e2_0_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24f197c2-883a-4803-9bc2-7d75065d5f0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot write to already existent path file:/Users/matthew.powers/data/delta_dv_G1_1e9_1e2_0_0 without setting OVERWRITE = 'true'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1398\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot write to already existent path file:/Users/matthew.powers/data/delta_dv_G1_1e9_1e2_0_0 without setting OVERWRITE = 'true'."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df.write.format(\"delta\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa291cc-e779-454d-a93f-b055b36c8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "delta_table.optimize().executeZOrderBy(\"id1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f9e7b44-6aca-4b95-8815-88fcdcae1f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable deletion vectors\n",
    "\n",
    "spark.sql(\n",
    "    f\"ALTER TABLE delta.`{delta_path}` SET TBLPROPERTIES ('delta.enableDeletionVectors' = true)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3fc2561a-b090-435f-b07d-314b3d7796c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(delta_path).createOrReplaceTempView(\"x1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "426a48d5-63cf-494f-91ff-367b3795f23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 ms, sys: 7.52 ms, total: 19.4 ms\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "delta_table = delta.DeltaTable.forPath(spark, delta_path)\n",
    "delta_table.delete(F.col(\"id1\") == \"id001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83122860-fabc-4d76-bcfd-9920bdfc877a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 196:==================================================>    (23 + 2) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|  id1|         v1|\n",
      "+-----+-----------+\n",
      "|id080| 2.997988E7|\n",
      "|id084|3.0005578E7|\n",
      "|id083|3.0005209E7|\n",
      "|id085|3.0010513E7|\n",
      "|id082|3.0007351E7|\n",
      "|id081|2.9988686E7|\n",
      "|id034|3.0010786E7|\n",
      "|id036|2.9994349E7|\n",
      "|id032|2.9986434E7|\n",
      "|id037|2.9996759E7|\n",
      "|id033|2.9983262E7|\n",
      "|id035|3.0003917E7|\n",
      "|id005|2.9993888E7|\n",
      "|id003|3.0003365E7|\n",
      "|id002|2.9996534E7|\n",
      "|id004| 3.001599E7|\n",
      "|id056|2.9987234E7|\n",
      "|id059|3.0010798E7|\n",
      "|id057|2.9991822E7|\n",
      "|id058|2.9999957E7|\n",
      "+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 21.3 ms, sys: 10.1 ms, total: 31.4 ms\n",
      "Wall time: 26.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\"select id1, sum(v1) as v1 from x1 group by id1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58b4fa43-3827-4065-9e17-fc961a138733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.4 ms, sys: 10 ms, total: 35.4 ms\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "delta_table = delta.DeltaTable.forPath(spark, delta_path)\n",
    "delta_table.delete((F.col(\"id2\") == F.lit(\"id030\")) & (F.col(\"id4\") == F.lit(22)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "679d1153-9b91-49b2-b536-d1156efe6fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 226:===================================================> (199 + 6) / 205]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|  id2|     sum_id4|\n",
      "+-----+------------+\n",
      "|id089|4.99702831E8|\n",
      "|id080|5.00174365E8|\n",
      "|id087|4.99700307E8|\n",
      "|id073|4.99578553E8|\n",
      "|id043|4.99827245E8|\n",
      "|id064|4.99800549E8|\n",
      "|id051|4.99712039E8|\n",
      "|id045|4.99602129E8|\n",
      "|id074|4.99709236E8|\n",
      "|id023|5.00068708E8|\n",
      "|id006|  4.999594E8|\n",
      "|id013| 5.0012219E8|\n",
      "|id055|4.99983587E8|\n",
      "|id099| 4.9972291E8|\n",
      "|id056| 5.0021648E8|\n",
      "|id052|4.99804639E8|\n",
      "|id093|4.99838447E8|\n",
      "|id034|4.99737564E8|\n",
      "|id075|4.99923346E8|\n",
      "|id036|4.99873408E8|\n",
      "+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 61.2 ms, sys: 20.9 ms, total: 82 ms\n",
      "Wall time: 20.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\"select id2, sum(id4) as sum_id4 from x0 group by id2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c8908-23ea-4d0b-a37d-76a9db891e50",
   "metadata": {},
   "source": [
    "## Purging Deletion Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dbe82b2-ef9b-4ffa-b5a1-55c28d5da2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"bob\", 1), (\"li\", 2), (\"leonard\", 3)]).toDF(\n",
    "    \"first_name\", \"id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c9b055f-dedf-4a4f-bff0-ade146d66a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = f\"{Path().absolute()}/tmp/some_ppl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bababa3-ee52-4f65-903f-b1a309b10215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1).write.format(\"delta\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f15885f-b1b0-4e57-9eff-6ef2ab61fea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    f\"ALTER TABLE delta.`{delta_path}` SET TBLPROPERTIES ('delta.enableDeletionVectors' = true)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7d8d1aa-5684-433e-b3ee-395154d0ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtmp/some_ppl\u001b[0m\n",
      "├── \u001b[01;34m_delta_log\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "│   └── \u001b[00m00000000000000000001.json\u001b[0m\n",
      "└── \u001b[00mpart-00000-1d16df58-7cbe-4608-9782-429f3a9bd639-c000.snappy.parquet\u001b[0m\n",
      "\n",
      "1 directory, 3 files\n"
     ]
    }
   ],
   "source": [
    "!tree tmp/some_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ffe3f4b-cb80-488d-9edf-47ca880c0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = delta.DeltaTable.forPath(spark, delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "574d4fea-cede-4e33-b93a-9c46e75426e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table.delete(F.col(\"id\") == F.lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49d7719c-adcb-4f6a-b0a5-237c78864838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtmp/some_ppl\u001b[0m\n",
      "├── \u001b[01;34m_delta_log\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000001.json\u001b[0m\n",
      "│   └── \u001b[00m00000000000000000002.json\u001b[0m\n",
      "├── \u001b[00mdeletion_vector_6a611095-5f85-4961-9394-950bfb49a4a0.bin\u001b[0m\n",
      "└── \u001b[00mpart-00000-1d16df58-7cbe-4608-9782-429f3a9bd639-c000.snappy.parquet\u001b[0m\n",
      "\n",
      "1 directory, 5 files\n"
     ]
    }
   ],
   "source": [
    "!tree tmp/some_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6ef33bf-e662-405a-9a54-38ac656a8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table.delete(F.col(\"id\") == F.lit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "699f8618-9e2b-4491-a0b0-59cee742ff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtmp/some_ppl\u001b[0m\n",
      "├── \u001b[01;34m_delta_log\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000001.json\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000002.json\u001b[0m\n",
      "│   └── \u001b[00m00000000000000000003.json\u001b[0m\n",
      "├── \u001b[00mdeletion_vector_6a611095-5f85-4961-9394-950bfb49a4a0.bin\u001b[0m\n",
      "├── \u001b[00mdeletion_vector_e2928f07-529a-48d7-9cc0-a97b4ce20582.bin\u001b[0m\n",
      "└── \u001b[00mpart-00000-1d16df58-7cbe-4608-9782-429f3a9bd639-c000.snappy.parquet\u001b[0m\n",
      "\n",
      "1 directory, 7 files\n"
     ]
    }
   ],
   "source": [
    "!tree tmp/some_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4613ca98-6640-4c74-8562-0f82ed52ba95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"REORG TABLE delta.`{delta_path}` APPLY (PURGE);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef3858dc-f065-4a70-8dca-b6f8620a8287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtmp/some_ppl\u001b[0m\n",
      "├── \u001b[01;34m_delta_log\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000001.json\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000002.json\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000003.json\u001b[0m\n",
      "│   └── \u001b[00m00000000000000000004.json\u001b[0m\n",
      "├── \u001b[00mdeletion_vector_6a611095-5f85-4961-9394-950bfb49a4a0.bin\u001b[0m\n",
      "├── \u001b[00mdeletion_vector_e2928f07-529a-48d7-9cc0-a97b4ce20582.bin\u001b[0m\n",
      "├── \u001b[00mpart-00000-1d16df58-7cbe-4608-9782-429f3a9bd639-c000.snappy.parquet\u001b[0m\n",
      "└── \u001b[00mpart-00000-40ba36d8-8de6-4728-8c0c-17ac407f5340-c000.snappy.parquet\u001b[0m\n",
      "\n",
      "1 directory, 9 files\n"
     ]
    }
   ],
   "source": [
    "!tree tmp/some_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dba4a6a4-9d26-448e-90fc-aba5472b63e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 3 files and directories in a total of 1 directories.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "delta_table.vacuum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b499b455-a459-493d-9e78-eaffa1471624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtmp/some_ppl\u001b[0m\n",
      "├── \u001b[01;34m_delta_log\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000001.json\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000002.json\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000003.json\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000004.json\u001b[0m\n",
      "│   ├── \u001b[00m00000000000000000005.json\u001b[0m\n",
      "│   └── \u001b[00m00000000000000000006.json\u001b[0m\n",
      "└── \u001b[00mpart-00000-40ba36d8-8de6-4728-8c0c-17ac407f5340-c000.snappy.parquet\u001b[0m\n",
      "\n",
      "1 directory, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree tmp/some_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d692fa-e305-4aa4-be73-84039225873a",
   "metadata": {},
   "source": [
    "## Deletion vector interop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2854f79-01a0-475a-9a75-719171521452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"bob\", 1), (\"li\", 2), (\"leonard\", 3)]).toDF(\n",
    "    \"first_name\", \"id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c0c9b93-0b69-4102-8ef5-d09544ae7a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = f\"{Path().absolute()}/tmp/some_ppl2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c6348e-e715-427d-bcd6-677b8d247fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 10:57:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df.repartition(1).write.format(\"delta\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17008f86-ba32-4f9c-8021-1ac602cf4077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    f\"ALTER TABLE delta.`{delta_path}` SET TBLPROPERTIES ('delta.enableDeletionVectors' = true)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97290998-cbaf-4c0c-a056-50ffcb854024",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = delta.DeltaTable.forPath(spark, delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c2f980c-27b2-4f90-8b73-3a4c2f792865",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table.delete(F.col(\"id\") == F.lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20afe308-098a-4377-be4c-55dbea62d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|first_name| id|\n",
      "+----------+---+\n",
      "|        li|  2|\n",
      "|   leonard|  3|\n",
      "+----------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 10:59:42 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /private/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/blockmgr-66edd5cb-b773-4f44-ab86-b65919e6fbb6. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /private/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/blockmgr-66edd5cb-b773-4f44-ab86-b65919e6fbb6\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 10:59:42 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /private/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/blockmgr-66edd5cb-b773-4f44-ab86-b65919e6fbb6/30. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /private/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/blockmgr-66edd5cb-b773-4f44-ab86-b65919e6fbb6/30\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 55559)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthew.powers/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/matthew.powers/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/matthew.powers/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/matthew.powers/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/matthew.powers/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/matthew.powers/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/Users/matthew.powers/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/matthew.powers/opt/miniconda3/envs/pyspark-340-delta-240/lib/python3.9/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbc009-e9bc-4c49-94cb-bddc92bcf642",
   "metadata": {},
   "source": [
    "Switch to Python deltalake to make sure that table is properly read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccfc8f0c-05eb-4709-a504-16ac69228c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from deltalake import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715444a5-8a7b-4061-9ada-29b3bd591d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = f\"{Path().absolute()}/tmp/some_ppl2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a798ac70-8c1d-4c05-a0e1-8914709645d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "DeltaProtocolError",
     "evalue": "The table's minimum reader version is 3but deltalake only supports up to version 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDeltaProtocolError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDeltaTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deltalake-0100/lib/python3.9/site-packages/deltalake/table.py:553\u001b[0m, in \u001b[0;36mDeltaTable.to_pandas\u001b[0;34m(self, partitions, columns, filesystem, filters)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_pandas\u001b[39m(\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    540\u001b[0m     partitions: Optional[List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     filters: Optional[FilterType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    544\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    545\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    Build a pandas dataframe using data from the DeltaTable.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    :param filters: A disjunctive normal form (DNF) predicate for filtering rows. If you pass a filter you do not need to pass ``partitions``\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pyarrow_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deltalake-0100/lib/python3.9/site-packages/deltalake/table.py:534\u001b[0m, in \u001b[0;36mDeltaTable.to_pyarrow_table\u001b[0;34m(self, partitions, columns, filesystem, filters)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    533\u001b[0m     filters \u001b[38;5;241m=\u001b[39m _filters_to_expression(filters)\n\u001b[0;32m--> 534\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pyarrow_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_table(columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mfilters)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deltalake-0100/lib/python3.9/site-packages/deltalake/table.py:478\u001b[0m, in \u001b[0;36mDeltaTable.to_pyarrow_dataset\u001b[0;34m(self, partitions, filesystem, parquet_read_options)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03mBuild a PyArrow Dataset using data from the DeltaTable.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m:return: the PyArrow dataset in PyArrow\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol()\u001b[38;5;241m.\u001b[39mmin_reader_version \u001b[38;5;241m>\u001b[39m MAX_SUPPORTED_READER_VERSION:\n\u001b[0;32m--> 478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DeltaProtocolError(\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe table\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms minimum reader version is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol()\u001b[38;5;241m.\u001b[39mmin_reader_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut deltalake only supports up to version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_SUPPORTED_READER_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    481\u001b[0m     )\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filesystem:\n\u001b[1;32m    484\u001b[0m     filesystem \u001b[38;5;241m=\u001b[39m pa_fs\u001b[38;5;241m.\u001b[39mPyFileSystem(\n\u001b[1;32m    485\u001b[0m         DeltaStorageHandler(\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mtable_uri(),\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_options,\n\u001b[1;32m    488\u001b[0m         )\n\u001b[1;32m    489\u001b[0m     )\n",
      "\u001b[0;31mDeltaProtocolError\u001b[0m: The table's minimum reader version is 3but deltalake only supports up to version 1."
     ]
    }
   ],
   "source": [
    "DeltaTable(delta_path).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf2a6b3-52c9-4541-96cc-ccfe96d00ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DeltaTable(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b0185f-8ba1-4105-9099-9658516ea2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProtocolVersions(min_reader_version=3, min_writer_version=7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.protocol()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8766d-cc93-4bf3-8b28-01c7fcbc883f",
   "metadata": {},
   "source": [
    "## Read DAT DV table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "977e4761-2032-4a8e-93eb-df0199aa16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/matthew.powers/data/out/reader_tests/generated/deletion_vectors/delta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea7b0e0-2f07-463d-8afc-266b7bc3107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/05 16:46:32 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|letter|int|      date|\n",
      "+------+---+----------+\n",
      "|     b|228|1978-12-01|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ed9d7-1a3d-4123-a7cb-8add72fd03c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-350-delta-320",
   "language": "python",
   "name": "pyspark-350-delta-320"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

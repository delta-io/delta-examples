{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "The Delta Lake [`replaceWhere`](https://mungingdata.com/delta-lake/updating-partitions-with-replacewhere/) option allows users to selectively apply updates to specific data partitions rather than to full lakes, which may result in significant speed gains. This notebook briefly illustrates the usage of `replaceWhere` option. For more details, see:\n",
    "- [Selectively updating Delta partitions with replaceWhere](https://mungingdata.com/delta-lake/updating-partitions-with-replacewhere/) (this notebook will be following the example from this blog)\n",
    "- [Selectively overwrite data with Delta Lake](https://docs.databricks.com/delta/selective-overwrite.html)\n",
    "- [Table batch reads and writes: overwrite](https://docs.delta.io/latest/delta-batch.html#overwrite)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+\n",
      "|first_name|last_name|  country|continent|\n",
      "+----------+---------+---------+---------+\n",
      "|   Ernesto|  Guevara|Argentina|     null|\n",
      "|  Vladimir|    Putin|   Russia|     null|\n",
      "|     Maria|Sharapova|   Russia|     null|\n",
      "|     Bruce|      Lee|    China|     null|\n",
      "|      Jack|       Ma|    China|     null|\n",
      "+----------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "df = spark.read.options(header='True', charset='UTF8')\\\n",
    "    .csv(\"../../data/people_countries.csv\")\\\n",
    "    .withColumn(\"continent\", lit(None).cast(StringType()))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition on Country\n",
    "Now we'll repartition the DataFrame on `country` and write it to disk in the Delta Lake format, partitioned by `country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/23 13:28:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "deltaPath = \"../../data/people_countries_delta/\"\n",
    "\n",
    "(df.repartition(col(\"country\")) \n",
    "  .write \n",
    "  .partitionBy(\"country\") \n",
    "  .format(\"delta\") \n",
    "  .mode(\"overwrite\") \n",
    "  .save(deltaPath)\n",
    "  )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function to add `continent` values to a DataFrame based on the value of `country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "def withContinent(df):\n",
    "    return df.withColumn(\n",
    "        \"continent\",\n",
    "        when(col(\"country\") == \"Russia\", \"Europe\")\n",
    "          .when(col(\"country\") == \"China\", \"Asia\")\n",
    "          .when(col(\"country\") == \"Argentina\", \"South America\")\n",
    "      )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where `replaceWhere` comes in. Suppose we only want to populate the `continent` column when `country == 'China'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(deltaPath)\n",
    "df = df.where(col(\"country\") == \"China\").transform(withContinent)\n",
    "\n",
    "(df.write\n",
    "  .format(\"delta\")\n",
    "  .option(\"replaceWhere\", \"country = 'China'\")\n",
    "  .mode(\"overwrite\")\n",
    "  .save(deltaPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+\n",
      "|first_name|last_name|country  |continent|\n",
      "+----------+---------+---------+---------+\n",
      "|Bruce     |Lee      |China    |Asia     |\n",
      "|Jack      |Ma       |China    |Asia     |\n",
      "|Ernesto   |Guevara  |Argentina|null     |\n",
      "|Vladimir  |Putin    |Russia   |null     |\n",
      "|Maria     |Sharapova|Russia   |null     |\n",
      "+----------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"add\": {\n",
      "        \"path\": \"country=China/part-00000-2f823649-f7af-45e7-95cc-fce354972434.c000.snappy.parquet\",\n",
      "        \"partitionValues\": {\n",
      "            \"country\": \"China\"\n",
      "        },\n",
      "        \"size\": 1002,\n",
      "        \"modificationTime\": 1687544927388,\n",
      "        \"dataChange\": true,\n",
      "        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"first_name\\\":\\\"Bruce\\\",\\\"last_name\\\":\\\"Lee\\\",\\\"continent\\\":\\\"Asia\\\"},\\\"maxValues\\\":{\\\"first_name\\\":\\\"Jack\\\",\\\"last_name\\\":\\\"Ma\\\",\\\"continent\\\":\\\"Asia\\\"},\\\"nullCount\\\":{\\\"first_name\\\":0,\\\"last_name\\\":0,\\\"continent\\\":0}}\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"remove\": {\n",
      "        \"path\": \"country=China/part-00000-5b20d31c-1a49-47f0-a5b1-0f2e5b422753.c000.snappy.parquet\",\n",
      "        \"deletionTimestamp\": 1687544926730,\n",
      "        \"dataChange\": true,\n",
      "        \"extendedFileMetadata\": true,\n",
      "        \"partitionValues\": {\n",
      "            \"country\": \"China\"\n",
      "        },\n",
      "        \"size\": 929\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../../data/people_countries_delta/_delta_log/00000000000000000001.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        if \"add\" in data or \"remove\" in data:\n",
    "            print(json.dumps(data, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only the `country=China/part-00000-87aebbc2-aff3-4bd6-b369-aa9aacbb93be.c000.snappy.parquet` file was modified. The other partitions were not.\n",
    "\n",
    "For more details, read the [blog post](https://mungingdata.com/delta-lake/updating-partitions-with-replacewhere/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-340-delta-240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
